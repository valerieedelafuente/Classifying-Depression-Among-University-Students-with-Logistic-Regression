---
title: "Final Step 3"
format: pdf
editor: visual
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
```

```{r, include=FALSE, message=FALSE}
# Load necessary packages
library(readr)
library(tidyverse)
library(naniar)
library(janitor)
library(knitr)
library(rmarkdown)
library(car)
library(caret)
#setwd("/Documents/PSTAT 100/finalproject/PSTAT-100-Project/")

# Load in the data
depression_data <- read.csv("student_depression_dataset.csv")
```

```{r, include=FALSE}
# Fix column names
depression_data <- depression_data %>% 
  clean_names() %>%
  rename(
    cum_gpa = cgpa,
    suicidal_thoughts = have_you_ever_had_suicidal_thoughts,
    fam_mental_illness = family_history_of_mental_illness
  )

# Fixing the `financial_stress` variable
depression_data <- depression_data %>%
  mutate(
    financial_stress = as.numeric(financial_stress), 
    # convert string numbers to integers
    financial_stress = case_when(
      financial_stress == "?" ~ NA,
      # convert "?" to NA values
      .default = financial_stress))

# Remove 3 rows with NA values
depression_data <- depression_data %>% na.omit()

# Factorizing the `gender` variable
depression_data$gender <- factor(depression_data$gender)

# Fixing the `city` variable to change invalid entries
depression_data <- depression_data %>%
  mutate(city = case_when(
    city == "Khaziabad" ~ "Ghaziabad",
    city == "Nalyan" ~ "Kalyan",
    city == "'Less Delhi'" ~ "Delhi",
    city == "'Less than 5 Kalyan'" ~ "Kalyan",
    city == "3.0" ~ "Other",
    city == "Saanvi" ~ "Other",
    city == "M.Tech" ~ "Other",
    city == "Bhavna" ~ "Other",
    city == "City" ~ "Other",
    city == "Mira" ~ "Other",
    city == "Harsha" ~ "Other",
    city == "Vaanya" ~ "Other",
    city == "Gaurav" ~ "Other",
    city == "Harsh" ~ "Other",
    city == "Reyansh" ~ "Other",
    city == "Kibara" ~ "Other",
    city == "Rashi" ~ "Other",
    city == "ME" ~ "Other",
    city == "M.Com" ~ "Other",
    city == "Mihir" ~ "Other",
    city == "Nalini" ~ "Other",
    city == "Nandini" ~ "Other",
    TRUE ~ city  # Leave valid entries as they are
  ))

# # Fixing the `profession` variable to change invalid entries
depression_data <- depression_data %>%
  mutate(profession = case_when(
    profession == "'Civil Engineer'" ~ "Civil Engineer",
    profession == "'UX/UI Designer'" ~ "UX/UI Designer",
    profession == "'Digital Marketer'" ~ "Digital Marketer",
    profession == "'Content Writer'" ~ "Content Writer",
    profession == "'Educational Consultant'" ~ "Educational Consultant",
    TRUE ~ profession # Leave valid entries as they are
  ))


# Fixing the `work_pressure` variable for proper scaling
depression_data <- depression_data %>%
  mutate(work_pressure = case_when(
    work_pressure == 0 ~ 0,
    work_pressure == 2 ~ 1,
    work_pressure == 5 ~ 3
  ))

# Fixing the `sleep_duration` variable to change invalid entries
depression_data <- depression_data %>% 
  mutate(sleep_duration = case_when(
    sleep_duration == "'5-6 hours'" ~ "5-6 hours",
    sleep_duration == "'Less than 5 hours'" ~ "Less than 5 hours",
    sleep_duration == "'7-8 hours'" ~ "7-8 hours",
    sleep_duration == "'More than 8 hours'" ~ "More than 8 hours",
    sleep_duration == "Others" ~ "Other"
  ))

# Factorizing the `sleep_duration` variable
depression_data <- depression_data %>%
  mutate(sleep_duration = factor(sleep_duration, 
                                 levels = c("Less than 5 hours", 
                                            "5-6 hours", 
                                            "7-8 hours", 
                                            "More than 8 hours", 
                                            "Other"),
                                 ordered = TRUE))

# Fixing the `dietary_habits` variable to change misspelling
depression_data <- depression_data %>% 
  mutate(dietary_habits = case_when(
    dietary_habits == "Others" ~ "Other",
    TRUE ~ dietary_habits
  ))

# Factorizing the `dietary_habits` variable
depression_data <- depression_data %>%
  mutate(dietary_habits = factor(dietary_habits,
                                 levels = c("Healthy", "Moderate", "Unhealthy",
                                            "Other"),
                                 ordered = TRUE))

# Fixing the `degree` variable to change invalid entries
depression_data <- depression_data %>%
  mutate(degree = case_when(
    degree == "'Class 12'" ~ "High School",
    degree == "Others" ~ "Other",  
    # Others could less than HS education or totally unknown. 
    .default = degree
  ))

# Factorizing the `degree variable`
degree_levels <- c(
  "High School",
  "BA", "BSc", "B.Com", "BCA", "B.Pharm", "B.Ed", "B.Tech", "BE", "BHM", "B.Arch", "BBA",
  "MA", "MSc", "MBA", "M.Com", "MCA", "M.Tech", "M.Ed", "M.Pharm", "MHM",
  "LLB", "LLM", "MD", "MBBS",
  "PhD",
  "Other"
)

depression_data <- depression_data %>%
  mutate(degree = factor(degree, levels = degree_levels, ordered = TRUE))

# Factorizing the `suicidal_thoughts` variable
depression_data$suicidal_thoughts <- factor(depression_data$suicidal_thoughts)

# Factorizing the `fam_mental_illness` variable
depression_data$fam_mental_illness <- factor(depression_data$fam_mental_illness)

# Turning the `depression` variable back to "yes" and "no" for visualization purposes
depression_data <- depression_data %>% 
  mutate(depression = case_when(
    depression == 0 ~ "No",
    depression == 1 ~ "Yes"
  ))

# Factorizing the `depression` variable
depression_data$depression <- factor(depression_data$depression)
```

## Abstract

## Introduction

## Hypothesis 1: Will

Our first hypothesis we want to test is that students with moderate to healthy dietary habits will have lower rates of depression than students with unhealthy dietary habits. We will fit a basic logistic regression model to predict the binary outcome variable, "depression", using our categorical variable "dietary_habits" as a predictor.

Before fitting our model, we should first fully understand the predictor we are working with as well as check some assumptions that need to be met for our model to function properly. Our predictor "Dietary Habits" is distributed as follows:

```{r, echo=FALSE}
kable(table(depression_data$dietary_habits), col.names = c("Habits", "Count"))
```

We have a slightly skewed distribution of responses between Healthy, Moderate, and Unhealthy dietary habits, as well as 12 observations that responded "Other". Because these 12 responses are a very small fraction of the overall data, we can remove these to simplify our model and our interpretations of it.

```{r, echo=FALSE}
depression_data <- depression_data %>% filter(
  dietary_habits != "Other"
)
```

Logistic Regression models have a few assumptions that must be met to perform properly. (Named elsewhere) We already have met, or assume we have met, most of the assumptions besides the one that says the predictor variable should be linear in the log-odds of the outcome variable. We can check this by plotting the log-odds against the levels of the predictor variable to assesst the linearity:

```{r, echo=FALSE}
depression_data$dietary_habits <- 
  as.numeric(factor(depression_data$dietary_habits, 
                    levels = c("Healthy", "Moderate", "Unhealthy")))

depression_data <- depression_data %>% mutate(
  depression = case_when(
    depression == "Yes" ~ 1,
    depression == "No" ~ 0
  )
)
```

```{r, echo=FALSE, message=FALSE}
logOddsDF <- depression_data %>%
  group_by(dietary_habits) %>%
  summarize(
    odds = mean(depression),
    logOdds = log(odds/(1-odds)))

ggplot(logOddsDF, aes(x = dietary_habits, y = logOdds)) +
  geom_point() + geom_smooth(method = 'lm', se = FALSE) +
  scale_x_continuous(
    breaks = c(1, 2, 3),
    labels = c("Healthy", "Moderate", "Unhealthy")) +  
  labs(title = "Log-Odds of Depression by Dietary Habits", y = "Log-Odds", x = "Dietary Habits")
```

There appears to be a strong linear relationship between the log-odds of depression by each level of Dietary Habits, so we can conclude that our data meets the necessary assumption and move on to fitting our model.

We will fit a logistic regression model with the logit link function that will predict the odds of having depression by each level of Dietary Habits. The results of the model are summarized in the following table:

```{r, echo=FALSE}
dietHabitsModel <- glm(depression ~ dietary_habits - 1, data = depression_data,
                       family = binomial(link = 'logit'))

logModelSum <- summary(dietHabitsModel)
kable(logModelSum$coefficients, format = 'markdown', digits = 42)
```

We see statistically significant results from our basic logistic regression model with just dietary habits as a predictor. Every level of dietary habits has statistically significant effects on the presence of depression. Our model predicts that Healthy dietary habits decrease the probability of depression by about 18.5%, while Moderate and Unhealthy dietary habits increase the probability of depression by about 24 and 88 percent respectively. This is a very strong result to start with, but we should check our model results in other ways as well.

We will first check to make sure the Dietary Habits categorical predictor is significant in predicting the presence of depression overall rather than just by the level of the variable. We will use the "anova" function to perform a Chi-Squared likelihood ratio test with a null hypothesis that the null model without Dietary Habits is sufficient in predicting depression, and an alternative hypothesis that Dietary Habits is significant to the model in terms of lowering the model's residual deviance. The results of the anova are the following:

```{r, echo=FALSE}
logModANOVA <- anova(dietHabitsModel)
kable(logModANOVA, digits = 1000)
```

This table shows us that when added to the null model, the Dietary Habits predictor reduced the model's residual deviance so greatly that the p-value for our likelihood ratio test is too small for R to display it. The true p-value is less than 2.2 \* 10\^-16, which is extremely tiny and reasonably rounded to 0. With this similarly significant result to the model results earlier, we can conclude that the Dietary Habits predictor as a whole is significant in predicting the presence of depression.

We can now check how accurate our model is at predicting the presence of depression using a Confusion Matrix. The Confusion Matrix will visualize the accuracy of our model in terms of true positive and negative rates in the diagonal cells, false positive rate in the bottom left cell (row 2, column 1), and false negative rate in the top right cell (row 1, col 2).

```{r, echo=FALSE}
pred_probs <- predict(dietHabitsModel, type = "response")

pred_class <- ifelse(pred_probs > 0.5, 1, 0)

pred_class <- factor(pred_class, levels = c(0, 1))
actual <- factor(depression_data$depression, levels = c(0, 1))

confMat <- confusionMatrix(pred_class, actual, positive = "1")

accuracy <- confMat$overall[1]

kable(confMat$table/(dim(depression_data)[1]), digits = 3)
```

Here we see our model has an accuracy of 0.61 or 61%, a false positive rate of 0.265 or 26.5%, and a false negative rate of 0.125 or 12.5%. This means our model with just Dietary Habits as a predictor is slightly better at predicting the presence of depression than a random guess.

Despite the fact that this model cannot predict the presence of depression with an impressive accuracy, the results of our model fit and surrounding hypothesis tests lead us to fail to reject our hypothesis that students with moderate to healthy dietary habits will have lower rates of depression than students with unhealthy dietary habits.

## Hypothesis 2

-   **Research Question**: Is there a correlation between the amount of sleep a student gets and the proportion of them that are depressed?

-   **Hypothesis**: Students who average more sleep per night will have lower rates of depression compared to students who average less.

### 1.1 Data Analysis

Based on the research question, hypothesis, and the characteristics of the data set, our analytical approach of choice for investigation is Classification; where we will construct a logistic regression model in an attempt to predict whether or not a student reports experiencing depression based on how many hours of sleep they average per night. This is the best method of choice because our outcome variable, whether or not the student is depressed, is a binary value and our predictor, the amount of sleep averaged per night, is categorical with an ordinal nature. Additionally, the method quantifies associations and predicts probabilities, and can be extended to control for other factors.

#### Data Processing

* **Data Cleaning**: In order to properly clean our data to test this hypothesis we must check to see if there are any missing values for both the outcome variable (`Depression`) and our chosen predictor variable (`Sleep Duration`): 

```{r,results='asis'}
#| echo: false
library(readr)
library(knitr)
library(dplyr)
library(tidyr)
library(kableExtra)
library(ggplot2)
library(broom)
depression <- read_csv("student_depression_dataset.csv")
missing_table <- depression %>% 
  select(Depression,`Sleep Duration`) %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count")
kable(missing_table,caption="NA Values")
```

* After aggregating the missing values data for both the `Depression` and `Sleep Duration` variables, we found that there are no `NA` values assigned to any observations for the respective variables. However this does not mean that there are no missing values present in the data. In order to investigate further we need to look at a frequency table of both variables:

```{r,results='asis'}
#| echo: false
#| layout-ncol: 2
sleeptable <- as.data.frame(table(depression$`Sleep Duration`))
colnames(sleeptable) <- c("Variable","Frequency")
depressiontable <- as.data.frame(table(depression$Depression))
colnames(depressiontable) <- c("Variable","Frequency")

kable(sleeptable,caption="Sleep Frequency")
kable(depressiontable,caption="Depression Frequency")
```

* As evident by the frequency tables, there are no missing values for the `Depression` variable in the data set, however there are 18 instances of `Others` being listed as values for the `Sleep Duration` variable, so we will treat those as instances with missing values and remove them from our data set.

```{r}
depression <- depression %>% filter(`Sleep Duration` != "Others")
```

* **Assumptions Required for Logistic Regression**: In order to use logistic regression to investigate our hypothesis, there are a few assumptions of the data that must be met in order for the model to be valid. That is (1), the outcome variable is binary (condition is met), (2), that the observations are independent of one another (condition is assumed based on how data was collected), (3), that the log-odds of the outcome is a linear function of the predictor variable, (4), that there is no multicollinearity (not of concern; only one variable involved in model), and (5), that there at least 10 events per predictor level (condition is met). In our case the only assumption that needs to be checked is the linearity of the log-odds.


```{r,out.width='45%',fig.align='center'}
depression$`Sleep Duration` <- as.numeric(factor(
  depression$`Sleep Duration`, levels = c("'Less than 5 hours'", "'5-6 hours'", "'7-8 hours'",
                                          "'More than 8 hours'")))
bin_summary <- depression %>%
  group_by(`Sleep Duration`) %>%
  summarize(
    mean_depression = mean(Depression),
    n = n(),
    log_odds = log(mean_depression/(1-mean_depression))
  ) %>%
  filter(!is.infinite(log_odds))

ggplot(bin_summary, aes(x = `Sleep Duration`, y = log_odds)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  scale_x_continuous(
    breaks = c(1, 2, 3, 4),
    labels = c("<5 hrs", "5-6 hrs", "7-8 hrs", ">8 hrs")
  ) +
  labs(title = "Log-Odds vs. Sleep Duration", y = "Log-Odds", x = "Sleep Duration") +
  theme_minimal()

depression <- depression %>% mutate(sleep_log = `Sleep Duration` * log(`Sleep Duration`))
model_bt <- glm(Depression~`Sleep Duration` + sleep_log, family=binomial,data=depression)
model_coef <- as.data.frame(summary(model_bt)$coefficients)
model_coef <- model_coef %>% select(-c(Estimate,`Std. Error`))
```

The Graph of Log-Odds vs. Sleep Duration shows us a somewhat clear linear relationship between Sleep Duration and the Log-Odds. To investigate further, we will use a Box-Tidwell Test and look at the p-value corresponding to `sleep_log` (the log of the `Sleep Duration` variable)

```{r}
kable(model_coef, digits = 3, caption="GLM Coefficient Estimates")
```

Based on the p-value of 0.948 corresponding to the `sleep_log` variable, at significance level $\alpha=0.05$, we fail to reject the null hypothesis that the log odds is a linear function of the `Sleep Duration` predictor variable, thus the (3) assumption is met and we can proceed to constructing our model.

### 2.1 The Model

* **Results**:
We use the `glm` function to generate the following logistic regression model for predicting the proportion of students reporting depression at the varying sleep ranges (and how that may be extrapolated to see how depression rates scale with sleep duration):

$$
\hat{DepressionProportion}=0.72583-0.15739SleepDuration
$$
```{r}
model <- glm(Depression ~ `Sleep Duration`, data = depression, family = binomial)
tidy_model <- tidy(model)
tidy_model <- tidy_model %>%
  mutate(p.value = formatC(p.value, format = "e", digits = 2))
kable(tidy_model,digits=3,caption="Logistic Model Table")
```




```{r,out.width='45%',fig.align='center'}

depression$predicted_prob <- predict(model, type = "response")
ggplot(depression, aes(x = `Sleep Duration`, y = predicted_prob)) +
  stat_summary(fun = mean, geom = "point", color = "blue", size = 3) +
  stat_summary(fun = mean, geom = "line", group = 1, color = "blue") +
  labs(title = "Predicted Probability of Depression vs. Sleep Duration",
       y = "Predicted Probability", x = "Sleep Duration") +
  theme_minimal() +
  scale_x_continuous(
    breaks = c(1, 2, 3, 4),
    labels = c("<5 hrs", "5-6 hrs", "7-8 hrs", ">8 hrs")
  )
```

* **Interpretation**: The model has an intercept coefficient of 0.72583, representing the average depression rate for students falling under the `0` Sleep Range (irrelevant as `1` is the reference group; representing less than 5 hours of sleep), and a `Sleep Duration` coefficient of -0.15739, representing the average change in depression probability when going from one sleep range to the next (in order). The p-values for both coefficients are <0.05, indicating statistical significance of the model.

* **Analyzing Performance: ROC curve and AUC**: To analyze the performance of the model we will investigate the ROC curve and area under the curve generated by the model:

```{r,out.width='45%',fig.align='center'}
library(pROC)
predicted_probs <- predict(model, type = "response")
roc_obj <- roc(depression$Depression, predicted_probs)

plot(roc_obj, main = "ROC Curve", col = "blue")
```

The ROC curve generated by the model is slightly above the diagonal (increasing and concave down), but not by much. Additionally, the AUC generated from the graph is 0.5494. This means that the prediction made by the model is relatively random, skewing slightly towards being a good model (correctly predicting the depression proportion based on Sleep Duration).

## Hypothesis 3: Hayden

Students with the highest collective reported stressors (Academic Pressure + Work Pressure + Financial Stress) will have higher rates of depression compared to students with lower collective reported stressors.

## Conclusion

## Recommendations
